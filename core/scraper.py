"""
+Leads â€” Google Maps scraper engine.
Searches for business categories across configured cities and extracts lead data
including WhatsApp numbers.

Supports proxy rotation via the ProxyRotator module.
"""

from __future__ import annotations

import asyncio
import logging
import re
from typing import Any

import asyncpg
from playwright.async_api import async_playwright, Page, TimeoutError as PWTimeout

from core.proxy import ProxyRotator
from db import upsert_lead

logger = logging.getLogger(__name__)

# Brazilian WhatsApp pattern: 55 + 2-digit DDD + 9-digit mobile (starts with 9)
# Also matches 8-digit landlines that some businesses list on WhatsApp Business
WHATSAPP_RE = re.compile(r"(?:\+?55)\s*\(?(\d{2})\)?\s*(9?\d{4})[\s\-]?(\d{4})")

# â”€â”€ Category lists by mode â”€â”€
ZAPPY_CATEGORIES = [
    # Core food
    "Restaurantes",
    "Pizzarias",
    "Lanchonetes",
    "Hamburguerias",
    "Padarias",
    "CafÃ©s",
    "Bares",
    "Sorveterias",
    # Brazilian specialties
    "AÃ§aÃ­",
    "AÃ§aiteria",
    "Churrascarias",
    "Tapiocarias",
    "Pastelarias",
    "Espetinhos",
    "Creperia",
    "Doceria",
    "Confeitaria",
    # Quick-service & delivery
    "Marmitaria",
    "Comida caseira",
    "Quentinha",
    "Food truck",
    "Delivery de comida",
    "Lanches",
    "Hot dog",
    "Salgaderia",
    "Cachorro quente",
    "Burger",
    # Drinks
    "Casa de sucos",
    "Distribuidora de bebidas",
    "Cervejaria",
    "Petiscaria",
    # Specialty cuisine
    "Comida japonesa",
    "Sushi",
    "Comida nordestina",
    "Galeto",
    "Frango assado",
    "Peixaria",
    "Marisqueira",
    "Buffet",
    "Self service",
    "Comida chinesa",
    "Comida mexicana",
    "Comida Ã¡rabe",
    "Comida italiana",
    "Comida vegana",
    "Comida vegetariana",
    "Comida fit",
    "Gelateria",
    "Casa de bolos",
    "DepÃ³sito de bebidas",
    # Extra food categories
    "Rotisseria",
    "Casa de carnes",
    "Frangos e assados",
    "Poke",
    "Temakeria",
    "Yakisoba",
    "Pastelaria",
    "Coxinharia",
    "Empadas e salgados",
    "Churros",
    "Waffle",
    "Panquecaria",
    "Forneria",
    "Esfiharia",
    "Comida baiana",
    "Comida mineira",
    "Comida goiana",
    "Comida peruana",
    "Comida portuguesa",
    "Restaurante popular",
    "Cantina",
    "BistrÃ´",
    "Gastropub",
    "Hamburgueria artesanal",
    "Pizza delivery",
    "Pizzaria delivery",
    "Lanchonete delivery",
    "Restaurante delivery",
    "Sushi delivery",
    "AÃ§aÃ­ delivery",
    "Marmita fitness",
    "Comida congelada",
    "Alimentos congelados",
    "CafÃ© da manhÃ£",
    "Brunch",
    "Casa de chÃ¡",
    "Cafeteria",
    "Loja de doces",
    "Bomboniere",
    "Chocolate artesanal",
    "Brownie",
    "Bolo no pote",
    "Bolo de rolo",
    "Tortas e bolos",
    "Salgados para festa",
    "Buffet infantil",
    "Buffet de festas",
    "Catering",
    "Loja de aÃ§aÃ­",
    "Frozen yogurt",
    "PicolÃ© artesanal",
    "Paleta mexicana",
    "Espetaria",
    "Churrasco",
    "Costela no bafo",
    "Picanha",
    "Frutos do mar",
    "Restaurante de peixe",
    "Tacos",
    "Burrito",
    "Kebab",
    "Shawarma",
    "Falafel",
    "Fish and chips",
    "Batata recheada",
    "SanduÃ­cheria",
    "Wrap",
    "Saladas",
    "Comida orgÃ¢nica",
    "Alimentos naturais",
    "Sucos naturais",
    "Smoothie",
    "Milkshake",
    "Bubble tea",
    "Ãgua de coco",
    "Bar de drinks",
    "Cocktailbar",
    "Wine bar",
    "Pub",
    "Choperia",
    "Adega",
    "Distribuidora de gÃ¡s",
]

LOJAKY_CATEGORIES = [
    # Fashion / Moda
    "Lojas de roupas",
    "Moda feminina",
    "Moda masculina",
    "Moda infantil",
    "Moda praia",
    "Moda plus size",
    "Moda evangÃ©lica",
    "Moda fitness",
    "Lojas de calÃ§ados",
    "Boutique",
    "BrechÃ³",
    "Loja de lingerie",
    "Loja de bolsas",
    "Loja de bijuterias",
    "Loja de acessÃ³rios",
    "Loja de tecidos",
    "Loja de uniformes",
    "Camisetas personalizadas",
    "AteliÃª de costura",
    "Sapataria",
    "Loja de chapÃ©us",
    # Jewelry / Joalheria
    "Joalheria",
    "Relojoaria",
    "Loja de semi joias",
    "Loja de prata",
    "Ã“tica e relojoaria",
    # Beauty / Beleza
    "SalÃµes de beleza",
    "SalÃ£o de cabelo",
    "Barbearias",
    "Barbearia premium",
    "Manicure e pedicure",
    "ClÃ­nica de estÃ©tica",
    "EstÃºdio de tatuagem",
    "Design de sobrancelhas",
    "Lojas de cosmÃ©ticos",
    "Perfumaria",
    "Loja de perfumes importados",
    "Loja de maquiagem",
    "ExtensÃ£o de cÃ­lios",
    "MicropigmentaÃ§Ã£o",
    "DepilaÃ§Ã£o a laser",
    "Spa",
    "Loja de produtos de beleza",
    "Loja de cabelos",
    "Perucas e apliques",
    "Nail designer",
    "Loja de esmaltes",
    "Produtos naturais",
    "Loja de produtos naturais",
    "Loja de suplementos",
    "EmpÃ³rio natural",
    # Pet
    "Pet shops",
    "Banho e tosa",
    "ClÃ­nica veterinÃ¡ria",
    "Pet shop e veterinÃ¡ria",
    "AcessÃ³rios para pets",
    "RaÃ§Ã£o e alimentos para pets",
    # Health / SaÃºde
    "FarmÃ¡cias",
    "Drogarias",
    "Ã“ticas",
    "ClÃ­nica odontolÃ³gica",
    "ConsultÃ³rio mÃ©dico",
    "ClÃ­nica de fisioterapia",
    "LaboratÃ³rio de anÃ¡lises",
    "ClÃ­nica dermatolÃ³gica",
    "Nutricionista",
    "PsicÃ³logo",
    "FonoaudiÃ³logo",
    "Loja de produtos ortopÃ©dicos",
    "Loja de equipamentos mÃ©dicos",
    "FarmÃ¡cia de manipulaÃ§Ã£o",
    # Fitness
    "Academias",
    "Studio de pilates",
    "Crossfit",
    "Escola de danÃ§a",
    "Escola de luta",
    "Yoga",
    "Personal trainer",
    "Loja de artigos esportivos",
    "Loja de suplementos esportivos",
    # Grocery / Mercados
    "Supermercado",
    "Mercadinho",
    "Minimercado",
    "Mercearia",
    "Loja de conveniÃªncia",
    "Hortifruti",
    "AtacadÃ£o",
    "Atacado e varejo",
    "EmpÃ³rio",
    "Casa de frios",
    "Loja de temperos",
    # Home / Casa
    "Loja de mÃ³veis",
    "Loja de material de construÃ§Ã£o",
    "Loja de tintas",
    "Loja de decoraÃ§Ã£o",
    "Loja de colchÃµes",
    "Loja de eletrodomÃ©sticos",
    "VidraÃ§aria",
    "Serralheria",
    "Marcenaria",
    "Loja de cortinas",
    "Loja de pisos e revestimentos",
    "Loja de iluminaÃ§Ã£o",
    "Loja de ferramentas",
    "Casa e jardim",
    "Loja de utilidades domÃ©sticas",
    "Loja de cama mesa e banho",
    "Tapetes e carpetes",
    "Persianas e cortinas",
    "Loja de ar condicionado",
    # Tech / EletrÃ´nicos
    "Loja de celulares",
    "Celulares e acessÃ³rios",
    "AssistÃªncia tÃ©cnica celular",
    "Conserto de celular",
    "Loja de capinhas",
    "Loja de eletrÃ´nicos",
    "Loja de informÃ¡tica",
    "AssistÃªncia tÃ©cnica notebook",
    "Loja de games",
    "Loja de drones",
    "Loja de som",
    "Loja de TVs",
    # Security / SeguranÃ§a
    "CFTV e cÃ¢meras",
    "Alarmes e seguranÃ§a",
    "Cercas elÃ©tricas",
    "PortÃµes automÃ¡ticos",
    # Electrical / ElÃ©trica
    "Materiais elÃ©tricos",
    "Loja de materiais hidrÃ¡ulicos",
    "Energia solar",
    "Eletricista",
    "Encanador",
    # Auto
    "AutopeÃ§as",
    "Oficina mecÃ¢nica",
    "Lava jato",
    "Borracharia",
    "Auto elÃ©trica",
    "Funilaria e pintura",
    "MotopeÃ§as",
    "Bicicletaria",
    "Som automotivo",
    "Insulfilm",
    "Pneus",
    "Estacionamento",
    "AcessÃ³rios automotivos",
    "Loja de baterias",
    # Retail misc
    "Lojas de varejo",
    "Papelarias",
    "Floricultura",
    "Loja de brinquedos",
    "Loja de presentes",
    "Armarinho",
    "Loja de embalagens",
    "Livraria",
    "Loja de artigos religiosos",
    "Loja de artigos para festas",
    "Casa de festas",
    "Aluguel de trajes",
    "Loja de malas e bolsas",
    "Tabacaria",
    "Sex shop",
    "Loja de pesca",
    "Loja de camping",
    "Loja de artigos militares",
    "Loja de instrumentos musicais",
    "Loja de artesanato",
    "Loja de quadros e molduras",
    "AntiquÃ¡rio",
    # Services / ServiÃ§os
    "Lavanderia",
    "Chaveiro",
    "GrÃ¡fica",
    "CartÃ³rio",
    "ImobiliÃ¡ria",
    "Contabilidade",
    "Escola de idiomas",
    "Auto escola",
    "Coworking",
    "FotÃ³grafo",
    "EstÃºdio fotogrÃ¡fico",
    "Dedetizadora",
    "Limpeza e conservaÃ§Ã£o",
    "CaÃ§amba e entulho",
    "MudanÃ§as e fretes",
    "Corretor de seguros",
    "Despachante",
    "EscritÃ³rio de advocacia",
    "Consultoria empresarial",
    "Marketing digital",
    "AgÃªncia de publicidade",
    "Web design",
    "Escola de cursos profissionalizantes",
    "Escola de informÃ¡tica",
    "Escola particular",
    "Creche",
    "ClÃ­nica de reforÃ§o escolar",
]

SEARCH_LOCATION = "Olinda, PE"

# â”€â”€ Locations by city â€” each city has its own neighborhoods â”€â”€
CITY_LOCATIONS = {
    "Olinda, PE": [
        "Casa Caiada",
        "Bairro Novo",
        "Rio Doce",
        "Jardim AtlÃ¢ntico",
        "Peixinhos",
        "Ouro Preto",
        "Cidade Tabajara",
        "Ãguas Compridas",
        "Amparo",
        "Carmo",
        "Varadouro",
        "Salgadinho",
        "Bultrins",
        "Fragoso",
        "Jardim Fragoso",
        "Sapucaia",
        "Monte",
        "Guadalupe",
        "Caixa D'Ãgua",
        "Alto da SÃ©",
        "Amaro Branco",
        "Bonsucesso",
        "SÃ£o Benedito",
        "Passarinho",
        "Alto da Bondade",
        "Jardim Brasil",
        "SÃ­tio Novo",
        "Aguazinha",
        "Pau Amarelo",
        "JatobÃ¡",
    ],
    "Camaragibe, PE": [
        "Centro",
        "Aldeia dos CamarÃ¡s",
        "Vera Cruz",
        "ChÃ£ de Cruz",
        "Tabatinga",
        "Bairro dos Estados",
        "Timbi",
        "Alberto Maia",
        "CÃ©u Azul",
        "Santa MÃ´nica",
        "Vila da FÃ¡brica",
        "Vale das Pedreiras",
        "Areeiro",
        "JoÃ£o Paulo II",
        "Borboleta",
        "Jardim Primavera",
        "Monte Alegre",
    ],
    "VÃ¡rzea, Recife, PE": [
        "VÃ¡rzea",
    ],
    "SÃ£o LourenÃ§o da Mata, PE": [
        "Centro",
        "Nova TiÃºma",
        "TiÃºma",
        "Matriz da Luz",
        "Pixete",
        "SÃ£o LÃ¡zaro",
        "Jardim TeresÃ³polis",
        "Dois Unidos",
    ],
}

# Keep backward compat
OLINDA_NEIGHBORHOODS = CITY_LOCATIONS["Olinda, PE"]

MAX_SCROLL_ATTEMPTS = 40
SELECTOR_RETRY = 3
SELECTOR_TIMEOUT_MS = 8_000


def _normalize_whatsapp(match: re.Match) -> str:
    """Normalize a matched phone number to 55DDDNUMBER format."""
    ddd, prefix, suffix = match.groups()
    number = prefix + suffix
    # Ensure mobile numbers have the leading 9 (total 9 digits after DDD)
    if len(number) == 8:
        number = "9" + number
    return f"55{ddd}{number}"


def _extract_whatsapp_numbers(text: str) -> list[str]:
    """Return de-duplicated WhatsApp-formatted numbers found in text."""
    matches = WHATSAPP_RE.finditer(text)
    seen: set[str] = set()
    results: list[str] = []
    for m in matches:
        normalised = _normalize_whatsapp(m)
        if normalised not in seen:
            seen.add(normalised)
            results.append(normalised)
    return results


def _classify_target_saas(mode: str) -> str:
    """Return the target SaaS based on the prospector mode."""
    return "Zappy" if mode == "zappy" else "Lojaky"


async def _retry_selector(page: Page, selector: str, retries: int = SELECTOR_RETRY) -> Any:
    """Wait for a selector with retry logic."""
    for attempt in range(1, retries + 1):
        try:
            element = await page.wait_for_selector(selector, timeout=SELECTOR_TIMEOUT_MS)
            return element
        except PWTimeout:
            logger.warning(
                "Selector '%s' not found (attempt %d/%d)", selector, attempt, retries
            )
            if attempt == retries:
                return None
            await asyncio.sleep(1)


async def _scroll_results(page: Page, feed_selector: str) -> None:
    """Scroll the results feed to load more listings."""
    for i in range(MAX_SCROLL_ATTEMPTS):
        try:
            end_marker = await page.query_selector("p.fontBodyMedium span:has-text('VocÃª chegou ao final')")
            if not end_marker:
                # English fallback
                end_marker = await page.query_selector("p.fontBodyMedium span:has-text(\"You've reached the end\")")
            if end_marker:
                logger.info("Reached end of results after %d scrolls", i)
                break
        except Exception:
            pass

        await page.evaluate(
            '(sel) => document.querySelector(sel)?.scrollBy(0, 800)',
            feed_selector,
        )
        await asyncio.sleep(1.5)


async def _scrape_category(
    page: Page,
    category: str,
    pool: asyncpg.Pool,
    mode: str = "zappy",
    location: str = SEARCH_LOCATION,
) -> int:
    """Scrape a single category from Google Maps. Returns number of leads inserted."""
    query = f"{category} em {location}"
    url = f"https://www.google.com/maps/search/{query.replace(' ', '+')}"
    logger.info("Scraping: %s", url)

    await page.goto(url, wait_until="domcontentloaded", timeout=30_000)
    await asyncio.sleep(3)

    # Accept cookies / consent if prompted
    try:
        consent_btn = await page.query_selector("button:has-text('Aceitar'), button:has-text('Accept')")
        if consent_btn:
            await consent_btn.click()
            await asyncio.sleep(1)
    except Exception:
        pass

    # Wait for results feed
    feed_selector = 'div[role="feed"]'
    feed = await _retry_selector(page, feed_selector)
    if not feed:
        logger.warning("No results feed found for category: %s", category)
        return 0

    # Scroll to load all results
    await _scroll_results(page, feed_selector)

    # Collect listing links
    listings = await page.query_selector_all(f'{feed_selector} a[href*="/maps/place/"]')
    total_listings = len(listings)
    logger.info("Found %d listings for '%s'", total_listings, category)

    inserted = 0

    for idx in range(total_listings):
        try:
            # Re-query listings each iteration to avoid stale references
            current_listings = await page.query_selector_all(
                f'{feed_selector} a[href*="/maps/place/"]'
            )
            if idx >= len(current_listings):
                logger.info("No more listings available at index %d", idx)
                break

            listing = current_listings[idx]

            # Extract business name from aria-label BEFORE clicking (most reliable)
            business_name = await listing.get_attribute("aria-label") or ""
            business_name = business_name.strip()
            if not business_name or business_name.lower() in ("resultados", "results", ""):
                # Fallback: try inner text of the listing
                try:
                    business_name = (await listing.inner_text()).strip().split("\n")[0]
                except Exception:
                    business_name = f"NegÃ³cio-{idx + 1}"

            # Click into the listing detail
            await listing.click()
            await asyncio.sleep(2.5)

            # Wait for the detail panel to load (look for the action buttons area)
            try:
                await page.wait_for_selector(
                    'button[data-item-id="phone"], button[data-item-id="address"], div[role="main"] h1',
                    timeout=5000,
                )
            except PWTimeout:
                pass

            # Try to get a better name from the detail panel h1
            try:
                # Try multiple selectors for the business name
                name_el = await page.query_selector('h1.DUwDvf')
                if not name_el:
                    name_el = await page.query_selector('div[role="main"] h1.fontHeadlineLarge')
                if not name_el:
                    name_el = await page.query_selector('div[role="main"] h1')
                    
                if name_el:
                    detail_name = (await name_el.inner_text()).strip()
                    # Only use if it's not a generic term
                    if detail_name and detail_name.lower() not in ("resultados", "results", "resultado", "result"):
                        business_name = detail_name
            except Exception:
                pass

            # Extract rating
            rating: float | None = None
            rating_el = await page.query_selector('span[aria-label*="estrela"], span[aria-label*="star"]')
            if rating_el:
                aria = await rating_el.get_attribute("aria-label") or ""
                rating_match = re.search(r"([\d,\.]+)", aria)
                if rating_match:
                    rating = float(rating_match.group(1).replace(",", "."))

            # Extract neighbourhood / address
            neighborhood: str | None = None
            addr_el = await page.query_selector('button[data-item-id="address"] div.fontBodyMedium')
            if addr_el:
                addr_text = await addr_el.inner_text()
                # Brazilian addresses: "Rua X, 123 - Bairro, Cidade - UF, CEP"
                # or: "Rua X, Bairro, Olinda - PE, 53020-140"
                # We need to extract the bairro (neighborhood) name
                parts = [p.strip() for p in addr_text.split(",")]
                neighborhood = None
                for part in parts:
                    cleaned = part.strip()
                    # Skip CEPs â€” many formats: 53020-140, 53.020-140, CEP 53020-140, etc.
                    if re.match(r'^(?:CEP\s*)?\d{2}\.?\d{3}-?\d{3}$', cleaned, re.IGNORECASE):
                        continue
                    # Skip anything that is mostly digits (CEP fragments, house numbers)
                    digits_only = re.sub(r'[\s\.\-]', '', cleaned)
                    if digits_only.isdigit() and len(digits_only) >= 5:
                        continue
                    # Skip parts with state abbreviation (Cidade - UF)
                    if re.search(r'\s*-\s*[A-Z]{2}$', cleaned):
                        continue
                    # Skip street numbers only
                    if re.match(r'^\d+$', cleaned):
                        continue
                    # Skip parts starting with common street prefixes
                    lower = cleaned.lower()
                    if any(lower.startswith(p) for p in ['r.', 'rua ', 'av.', 'av ', 'rod.', 'rod ', 
                            'travessa', 'tv.', 'estrada', 'alameda', 'al.', 'praÃ§a', 'pÃ§.',
                            'largo ', 'beco ', 'vila ', 'conj.', 'conjunto ', 'lot.', 'loteamento ']):
                        continue
                    # Skip known city names that could confuse extraction
                    if lower in ('olinda', 'recife', 'camaragibe', 'sÃ£o lourenÃ§o da mata', 'jaboatÃ£o',
                                 'jaboatÃ£o dos guararapes', 'paulista', 'brasil', 'brazil'):
                        continue
                    # Check for "123 - Bairro" pattern (number dash name)
                    dash_match = re.match(r'^\d+\s*-\s*(.+)$', cleaned)
                    if dash_match:
                        neighborhood = dash_match.group(1).strip()
                        break
                    # Otherwise this part might be the bairro
                    if len(cleaned) > 2 and not cleaned.isdigit():
                        neighborhood = cleaned
                        # Don't break - keep looking for a better match (after dash)

            # Extract ONLY the business's own phone number (not reviews/ads)
            # Strategy: use the phone button in the contact info section
            whatsapp_numbers: list[str] = []
            phone_el = await page.query_selector('button[data-item-id*="phone"] div.fontBodyMedium')
            if phone_el:
                phone_text = await phone_el.inner_text()
                whatsapp_numbers = _extract_whatsapp_numbers(phone_text)[:1]

            # Fallback: check for a phone link (tel:) in the action buttons
            if not whatsapp_numbers:
                phone_links = await page.query_selector_all('a[href^="tel:"]')
                for pl in phone_links[:1]:
                    href = await pl.get_attribute("href") or ""
                    nums = _extract_whatsapp_numbers(href)
                    if nums:
                        whatsapp_numbers = nums[:1]
                        break

            target = _classify_target_saas(mode)

            # Save business - with or without phone
            if whatsapp_numbers:
                # Has phone(s) - save each
                for wa in whatsapp_numbers:
                    was_inserted = await upsert_lead(
                        pool,
                        business_name=business_name,
                        whatsapp=wa,
                        neighborhood=neighborhood,
                        category=category,
                        google_rating=rating,
                        target_saas=target,
                    )
                    if was_inserted:
                        inserted += 1
                        logger.info("  â†’ Lead: %s | %s | %s", business_name, wa, neighborhood)
            else:
                # No phone found - save anyway for manual enrichment
                was_inserted = await upsert_lead(
                    pool,
                    business_name=business_name,
                    whatsapp=None,
                    neighborhood=neighborhood,
                    category=category,
                    google_rating=rating,
                    target_saas=target,
                )
                if was_inserted:
                    inserted += 1
                    logger.info("  â†’ Lead (no phone): %s | %s", business_name, neighborhood)

            # Go back to results
            await page.go_back(wait_until="domcontentloaded", timeout=15_000)
            await asyncio.sleep(2)

        except Exception as exc:
            logger.error("Error scraping listing %d in '%s': %s", idx, category, exc)
            # Try to recover by navigating back to the search
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=15_000)
                await asyncio.sleep(3)
                # Re-wait for feed
                await _retry_selector(page, feed_selector)
                await _scroll_results(page, feed_selector)
            except Exception:
                break

    return inserted


async def run_scraper(
    pool: asyncpg.Pool,
    proxy_rotator: ProxyRotator | None = None,
    mode: str = "zappy",
    scrape_cities: list[str] | None = None,
    custom_categories: list[str] | None = None,
    custom_neighborhoods: list[str] | None = None,
    disabled_neighborhoods: dict[str, list[str]] | None = None,
) -> int:
    """
    Main entry point for the scraper.
    Launches Playwright, iterates over search categories, and returns total inserts.
    Mode selects which categories to scrape: 'zappy' (food) or 'lojaky' (retail).
    scrape_cities filters which cities to scrape (empty/None = all).
    custom_categories: extra categories added from dashboard.
    custom_neighborhoods: extra neighborhoods added from dashboard.
    disabled_neighborhoods: dict of {city: [neighborhoods to skip]}.
    """
    total_inserted = 0

    proxy_config = None
    if proxy_rotator:
        pc = proxy_rotator.next()
        if pc:
            proxy_config = pc.to_playwright_dict()
            logger.info("Using proxy: %s", pc.server)

    async with async_playwright() as pw:
        launch_args = {
            "headless": True,
            "args": [
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--disable-setuid-sandbox",
                "--lang=pt-BR",
            ],
        }
        if proxy_config:
            launch_args["proxy"] = proxy_config

        browser = await pw.chromium.launch(**launch_args)
        context = await browser.new_context(
            locale="pt-BR",
            geolocation={"latitude": -8.0089, "longitude": -34.8553},
            permissions=["geolocation"],
            viewport={"width": 1280, "height": 900},
        )
        page = await context.new_page()

        categories = list(ZAPPY_CATEGORIES if mode == "zappy" else LOJAKY_CATEGORIES)
        target_saas = _classify_target_saas(mode)

        # Add custom categories from dashboard
        if custom_categories:
            for cc in custom_categories:
                if cc.strip() and cc.strip() not in categories:
                    categories.append(cc.strip())

        # Filter cities if specified
        cities_to_scrape = dict(CITY_LOCATIONS)
        if scrape_cities:
            cities_to_scrape = {
                city: list(neighborhoods)
                for city, neighborhoods in CITY_LOCATIONS.items()
                if any(sc.lower() in city.lower() for sc in scrape_cities)
            }
            if not cities_to_scrape:
                logger.warning("No matching cities found for: %s", scrape_cities)
                cities_to_scrape = dict(CITY_LOCATIONS)

        # Add custom neighborhoods to each active city
        if custom_neighborhoods:
            for city in cities_to_scrape:
                for cn in custom_neighborhoods:
                    cn = cn.strip()
                    if cn and cn not in cities_to_scrape[city]:
                        cities_to_scrape[city].append(cn)

        # Remove disabled neighborhoods per city
        if disabled_neighborhoods:
            for city in cities_to_scrape:
                disabled = disabled_neighborhoods.get(city, [])
                if disabled:
                    before = len(cities_to_scrape[city])
                    cities_to_scrape[city] = [
                        n for n in cities_to_scrape[city] if n not in disabled
                    ]
                    logger.info(
                        "City '%s': %d neighborhoods disabled, %d remaining",
                        city, before - len(cities_to_scrape[city]), len(cities_to_scrape[city]),
                    )

        # Build search locations: city-wide + each neighbourhood
        locations = []
        for city, neighborhoods in cities_to_scrape.items():
            locations.append(city)  # City-wide search
            for n in neighborhoods:
                locations.append(f"{n}, {city}")
        
        total_queries = len(categories) * len(locations)
        city_names = list(cities_to_scrape.keys())
        logger.info(
            "Mode: %s â€” %d categories Ã— %d locations (%s) = %d queries",
            mode, len(categories), len(locations), ", ".join(city_names), total_queries,
        )

        query_num = 0
        BROWSER_RESTART_INTERVAL = 500  # Restart browser every N queries to avoid memory crashes
        for category in categories:
            for location in locations:
                query_num += 1
                
                # Restart browser periodically to prevent memory exhaustion
                if query_num % BROWSER_RESTART_INTERVAL == 0:
                    logger.info("ðŸ”„ Restarting browser at query %d/%d to free memory", query_num, total_queries)
                    await page.close()
                    await context.close()
                    await browser.close()
                    browser = await pw.chromium.launch(**launch_args)
                    context = await browser.new_context(
                        locale="pt-BR",
                        geolocation={"latitude": -8.0089, "longitude": -34.8553},
                        permissions=["geolocation"],
                        viewport={"width": 1280, "height": 900},
                    )
                    page = await context.new_page()
                
                try:
                    count = await _scrape_category(
                        page, category, pool, mode=mode, location=location,
                    )
                    total_inserted += count
                    # Only log if we actually found new leads (reduce log spam)
                    if count:
                        logger.info(
                            "[%d/%d] '%s' @ %s: %d new leads",
                            query_num, total_queries, category, location, count,
                        )
                except Exception as exc:
                    logger.error(
                        "[%d/%d] Failed '%s' @ %s: %s",
                        query_num, total_queries, category, location, exc,
                    )

        await browser.close()

    logger.info("Scraping cycle complete â€” %d new leads inserted", total_inserted)
    return total_inserted
